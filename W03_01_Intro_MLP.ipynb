{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP0SFycZI2nPOVf4TVryjQL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvd42/Classification_Model-SVM-Logistic_Regression-/blob/master/W03_01_Intro_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOMdMgG8h88o"
      },
      "source": [
        "# Simple MLP in PyTorch\r\n",
        "\r\n",
        "In this notebook we will detail how to create and train a multilayer perceptron using pytorch. We will go through:\r\n",
        " \r\n",
        "1. Two different ways of creating an MLP\r\n",
        "2. Create a standard training loop\r\n",
        "3. Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIr2ty0tFA4C"
      },
      "source": [
        "import torch #should be installed by default in any colab notebook\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWLIxo9Oigfo"
      },
      "source": [
        "# If this cell fails you need to change the runtime of your colab notebook to GPU\r\n",
        "# Go to Runtime -> Change Runtime Type and select GPU\r\n",
        "assert torch.cuda.is_available(), \"GPU is not enabled\"\r\n",
        "\r\n",
        "# use gpu if available\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI_YXyigdTUC"
      },
      "source": [
        "# Data\n",
        "\n",
        "Before training we need data! So lets create an artificial dataset for our model to learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j02Wymqldd76",
        "cellView": "form"
      },
      "source": [
        "#@title The code in this cell defines a function to generate training and validation data. Just hit the play button before continuing\n",
        "def generate_data(n_samples, generator, regression=False, noise_scale=2, **kwargs):\n",
        "\n",
        "    x_train, y_train = generator(n_samples, **kwargs) # training data\n",
        "\n",
        "    if \"noise\" in kwargs:\n",
        "        kwargs[\"noise\"] *= noise_scale\n",
        "    \n",
        "    x_val, y_val = generator(n_samples, **kwargs)\n",
        "\n",
        "    \n",
        "\n",
        "    # Plot the data\n",
        "    fig, ax = plt.subplots(1, 2)\n",
        "    ax[0].set_title(\"Training Data\") \n",
        "    ax[1].set_title(\"Validation Data\")\n",
        "\n",
        "    if regression:\n",
        "        ax[0].scatter(x_train, y_train, cmap=plt.cm.coolwarm)\n",
        "        ax[1].scatter(x_val, y_val, cmap=plt.cm.coolwarm)\n",
        "    else:\n",
        "        ax[0].scatter(x_train[:, 0], x_train[:, 1], c=y_train, cmap=plt.cm.coolwarm)\n",
        "        ax[1].scatter(x_val[:, 0], x_val[:, 1], c=y_val, cmap=plt.cm.coolwarm)\n",
        "\n",
        "\n",
        "    return x_train, y_train, x_val, y_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xelf76G1mWHv"
      },
      "source": [
        "# find more datasets at https://scikit-learn.org/stable/modules/classes.html#samples-generator\r\n",
        "from sklearn.datasets import make_blobs\r\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)\r\n",
        "\r\n",
        "N = 1000 # number of sample\r\n",
        "x_train, y_train, x_val, y_val = generate_data([N, N], make_blobs, centers=[[0, 0.5], [0, -0.5]], random_state=42, cluster_std=0.2)\r\n",
        "\r\n",
        "#Convert the data from numpy arrays into PyTorch tensors\r\n",
        "x_train = torch.from_numpy(x_train).float()\r\n",
        "y_train = torch.from_numpy(y_train)\r\n",
        "x_val = torch.from_numpy(x_val).float()\r\n",
        "y_val = torch.from_numpy(y_val)\r\n",
        "\r\n",
        "# move data to gpu if available\r\n",
        "x_train = x_train.to(device)\r\n",
        "y_train = y_train.to(device)\r\n",
        "\r\n",
        "x_val = x_val.to(device)\r\n",
        "y_val = y_val.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eia08EI34Dug"
      },
      "source": [
        "#@title The code in this cell is just for visualization no need to worry about it. Just hit the play button before continuing { display-mode: \"form\" }\n",
        "# Dont worry about the code in this cell it is \n",
        "# just for visualization purposes you dont need to understand or edit it yet\n",
        "def plot_model(x, y, model, axis):\n",
        "\n",
        "    mesh = torch.arange(-2, 2, 0.01)\n",
        "\n",
        "    xx, yy = torch.meshgrid(mesh, mesh)\n",
        "    with torch.no_grad():\n",
        "        data = torch.from_numpy(np.vstack((xx.reshape(-1), yy.reshape(-1))).T).float()\n",
        "        Z = model(data.cuda().detach())\n",
        "    Z = Z.max(1)[1].reshape(xx.shape)\n",
        "    axis.contourf(xx, yy, Z.cpu(), cmap=plt.cm.coolwarm, alpha=0.3)\n",
        "    axis.scatter(x[:, 0].cpu(), x[:, 1].cpu(), c=y.cpu(), s=20, cmap=plt.cm.coolwarm)\n",
        "    axis.get_xaxis().set_ticks([])\n",
        "    axis.get_yaxis().set_ticks([])\n",
        "\n",
        "\n",
        "def plot_regressor(x, y, model, axis):\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        y_pred = model(x).detach()\n",
        "    axis.scatter(x.cpu(), y.cpu())\n",
        "    axis.plot(x.cpu(), y_pred.cpu(), 'r-', lw=5, label=\"Model Prediction\")\n",
        "    axis.get_xaxis().set_ticks([])\n",
        "    axis.get_yaxis().set_ticks([])\n",
        "\n",
        "def draw_plots(x, y, model, losses, visualize_surface, visualize_regressor):\n",
        "\n",
        "    if visualize_surface and visualize_regressor:\n",
        "        raise ValueError(\"Expected only one of 'visualize_error' or 'visualize_regressor' to be True.\")\n",
        "\n",
        "    if visualize_surface:\n",
        "        fig, ax = plt.subplots(1, 2)\n",
        "        ax[0].set_title(\"Output Space\")\n",
        "        ax[1].set_title(\"Losses\")\n",
        "        ax[1].plot(losses[\"train\"], label=\"training loss\")\n",
        "        ax[1].plot(losses[\"val\"], label=\"validation loss\")\n",
        "        ax[1].set_xlabel(\"Epoch\")\n",
        "        plot_model(x_val, y_val, model, ax[0])\n",
        "    \n",
        "    elif visualize_regressor:\n",
        "        fig, ax = plt.subplots(1, 2)\n",
        "        ax[0].set_title(\"Validation Data\")\n",
        "        ax[1].set_title(\"Losses\")\n",
        "        ax[1].plot(losses[\"train\"], label=\"training loss\")\n",
        "        ax[1].plot(losses[\"val\"], label=\"validation loss\")\n",
        "        ax[1].set_xlabel(\"Epoch\")\n",
        "        plot_regressor(x_val, y_val, model, ax[0])\n",
        "\n",
        "    else:\n",
        "        plt.plot(losses[\"train\"], label=\"training loss\")\n",
        "        plt.plot(losses[\"val\"], label=\"validation loss\")\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.pause(0.000001)\n",
        "    plt.show()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlFN480ikiJq"
      },
      "source": [
        "Now that these basic concepts are out of the way and we have our data lets take a look at how to create a simple MLP in PyTorch\n",
        "\n",
        "The ```torch.nn``` [package](https://pytorch.org/docs/stable/nn.html) is the one containing all of the neural network related layers, operations etc.\n",
        "\n",
        "In order to create a simple MLP we can define a sequential module like so:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(nn.Linear(inp_dim, hidden_dim),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_dim, output_dim))\n",
        "```\n",
        "So now we can feed data into this model and the ```nn``` module will take care of the forward pass.\n",
        "\n",
        "Some more complicated acrchitectures will require you to detail the forward pass of the model. This will require creating a class that inherits from ```nn.Module``` and define its ```forward``` and ```__init__``` methods.\n",
        "\n",
        "```\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fc1 = nn.Linear(inp_dim, hidden_dim),\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.fc1(x)\n",
        "        out = torch.relu()\n",
        "\n",
        "        return self.fc2(out)\n",
        "```\n",
        "\n",
        "Notice how this approach allows more flexibility since we can insert ANY python statement in the forward method, such as ```print```, ```if else```, etc.\n",
        "\n",
        "Since our model for this first session is very simple we will use the first approach in this notebook, but we encourage you to try your own model using the second approach.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMeMyAHVw8yp"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n_iIYfRtUiz"
      },
      "source": [
        "def validate(criterion, model):\n",
        "\n",
        "    # set model in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad(): # do not compute gradients for validation\n",
        "        y_pred = model(x_val)\n",
        "\n",
        "\n",
        "    # compute loss and accuracy \n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "    val_loss = criterion(y_pred.squeeze(), y_val)\n",
        "    val_acc = (y_val == predicted).sum().float() / len(y_val)\n",
        "\n",
        "    return val_loss, val_acc\n",
        "\n",
        "\n",
        "\n",
        "# Training loop\n",
        "def train(criterion, model, optimizer, epochs=300, visualize_surface=False, visualize_regressor=False):\n",
        "    \n",
        "    losses = {\"train\": [], \"val\": []}\n",
        "\n",
        "    for t in range(epochs):\n",
        "        \n",
        "        # activate training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Feed forward to get the logits\n",
        "        y_pred = model(x_train)\n",
        "    \n",
        "        # Compute the loss and accuracy\n",
        "        loss = criterion(y_pred.squeeze(), y_train)\n",
        "        score, predicted = torch.max(y_pred, 1)\n",
        "        train_acc = (y_train == predicted).sum().float() / len(y_train)\n",
        "        print(\"Training: [EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), train_acc))\n",
        "        losses[\"train\"].append(loss)\n",
        "        \n",
        "        # zero the gradients before running\n",
        "        # the backward pass.\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Backward pass to compute the gradient\n",
        "        # of loss w.r.t our learnable params. \n",
        "        loss.backward()\n",
        "\n",
        "        # Update params\n",
        "        optimizer.step()\n",
        "\n",
        "        # Run model on validation data\n",
        "        val_loss, val_acc = validate(criterion, model)\n",
        "        print(\"Validation: [EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, val_loss.item(), val_acc))\n",
        "        losses[\"val\"].append(val_loss)\n",
        "\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "        draw_plots(x_val, y_val, model, losses, visualize_surface, visualize_regressor)\n",
        "\n",
        "    print(\"Training: [EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), train_acc))\n",
        "    print(\"Validation: [EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, val_loss.item(), val_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyEjLgfjrWzs"
      },
      "source": [
        "## Linear Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAoS_kiRZPYT"
      },
      "source": [
        "import torch.nn as nn # nn package to create our linear model\n",
        "\n",
        "learning_rate = 1e-3\n",
        "lambda_l2 = 1e-5\n",
        "torch.manual_seed(0) # seed for reproductibility\n",
        "\n",
        "D = 2  # input dimensions\n",
        "C = 2  # num_classes\n",
        "H = 100  # num_hidden_units\n",
        "\n",
        "# each Linear module has a weight and bias\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(D, H),\n",
        "    nn.Linear(H, C)\n",
        ")\n",
        "\n",
        "# move model to gpu if available\n",
        "model.to(device)\n",
        "\n",
        "# nn package also has different loss functions.\n",
        "# we use cross entropy loss for our classification task\n",
        "criterion = torch.nn.CrossEntropyLoss() # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
        "\n",
        "# we use the optim package to apply\n",
        "# stochastic gradient descent for our parameter updates\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=lambda_l2) # built-in L2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Prc0qFO0SR5"
      },
      "source": [
        "### Run Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psBKsIFAtkXQ"
      },
      "source": [
        "# Run training\n",
        "# You can set visualize_surface to False to omit the visualization of the output space. The training will be much faster this way\n",
        "train(criterion, model, optimizer, visualize_surface=True)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpZG0az2slrJ"
      },
      "source": [
        "# Lets try with a more complicated dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUr0VCRVskfy"
      },
      "source": [
        "def make_spiral(n_points, noise=0.5):\r\n",
        "\r\n",
        "    n = np.sqrt(np.random.rand(n_points, 1)) * 780 * (2*np.pi)/360\r\n",
        "    d1x = -np.cos(n)*n + np.random.rand(n_points, 1) * noise\r\n",
        "    d1y = np.sin(n)*n + np.random.rand(n_points, 1) * noise\r\n",
        "\r\n",
        "    x, y = (np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x + 1, -d1y)))),\r\n",
        "            np.hstack((np.zeros(n_points), np.ones(n_points))).astype(\"int\"))\r\n",
        "\r\n",
        "    x = (x - x.mean()) / x.std()\r\n",
        "\r\n",
        "    return x, y\r\n",
        "\r\n",
        "N = 1000 # number of samples\r\n",
        "x_train, y_train, x_val, y_val = generate_data(N, make_spiral, noise=0.5)\r\n",
        "\r\n",
        "#Convert the data from numpy arrays into PyTorch tensors\r\n",
        "x_train = torch.from_numpy(x_train).float()\r\n",
        "y_train = torch.from_numpy(y_train)\r\n",
        "x_val = torch.from_numpy(x_val).float()\r\n",
        "y_val = torch.from_numpy(y_val)\r\n",
        "\r\n",
        "# move data to gpu if available\r\n",
        "x_train = x_train.to(device)\r\n",
        "y_train = y_train.to(device)\r\n",
        "\r\n",
        "x_val = x_val.to(device)\r\n",
        "y_val = y_val.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MVjaTWN0G00"
      },
      "source": [
        "### Run Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGRRXLV1tZDE"
      },
      "source": [
        "# Run training\r\n",
        "# You can set visualize_surface to False to omit the visualization of the output space. The training will be much faster this way\r\n",
        "train(criterion, model, optimizer, visualize_surface=True)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgRziMgUIegD"
      },
      "source": [
        "### Homework\n",
        "\n",
        "\n",
        "A) As you can see the model we created is not able to learn a more complex dataset at all. Change the architecture size, the number of neurons the layers, etc and create a model capable of doing so.\n",
        "\n",
        "B) Using the data generated in the cell below modify the training code to solve a regression problem.\n",
        "\n",
        "\n",
        "> Hints\n",
        "\n",
        "\n",
        "* Note that the generated dataset only has one feature\n",
        "* You can use the same training and validation functions with some slight modifications since it doesn't make sense to calculate accuracy in a regression problem\n",
        "* You can find different training criteria (a.k.a loss functions) in PyTorch [docs](https://pytorch.org/docs/stable/nn.html#loss-functions)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWgDVJRYrsi2"
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "N = 1000 # number of samples\n",
        "x_train, y_train, x_val, y_val = generate_data(N, make_regression, regression=True, noise=5, n_features=1, random_state=52)\n",
        "\n",
        "#Convert the data from numpy arrays into PyTorch tensors\n",
        "x_train = torch.from_numpy(x_train).float()\n",
        "y_train = torch.from_numpy(y_train).float()\n",
        "x_val = torch.from_numpy(x_val).float()\n",
        "y_val = torch.from_numpy(y_val).float()\n",
        "\n",
        "# move data to gpu if available\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "\n",
        "x_val = x_val.to(device)\n",
        "y_val = y_val.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQhb6UUKt1RG"
      },
      "source": [
        "### More Homework\r\n",
        "\r\n",
        "C) Using the data generated in the cell below modify the model to tackle a non-binary classification problem\r\n",
        "\r\n",
        "D) Using a Simple Model modify the optimizer to fit the data from question A). Explain the effect of LR, Momentum etc. If you changed the optimizer explain what the  optimizer you chose does and why does it work better or worse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT589ljIyhEx"
      },
      "source": [
        "# find more datasets at https://scikit-learn.org/stable/modules/classes.html#samples-generator\r\n",
        "from sklearn.datasets import make_blobs\r\n",
        "\r\n",
        "N = 1000 # number of samples\r\n",
        "x_train, y_train, x_val, y_val = generate_data([N, N, N], make_blobs, centers=[[0, 0.5], [0, -0.5], [-0.5, -0.5]], random_state=12, cluster_std=0.2)\r\n",
        "\r\n",
        "#Convert the data from numpy arrays into PyTorch tensors\r\n",
        "x_train = torch.from_numpy(x_train).float()\r\n",
        "y_train = torch.from_numpy(y_train)\r\n",
        "x_val = torch.from_numpy(x_val).float()\r\n",
        "y_val = torch.from_numpy(y_val)\r\n",
        "\r\n",
        "# move data to gpu if available\r\n",
        "x_train = x_train.to(device)\r\n",
        "y_train = y_train.to(device)\r\n",
        "\r\n",
        "x_val = x_val.to(device)\r\n",
        "y_val = y_val.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}